{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26c3e84-7dd3-456c-a88c-977cbeee6d88",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "For the classification of hypoglycemia, euglycemia and hyperglycemia from MIMIC IV electronic health records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eaa05a-6c4e-421d-9f0c-d8634a5dd5c2",
   "metadata": {},
   "source": [
    "### chartEventsPred run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9789f9fc-7cdc-46e8-94f5-498a97d71dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label is an object, converting to category...\n",
      "charttime is an object, converting to category...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "data = pd.read_csv('./chartEventsPred.csv', delimiter=',')\n",
    "\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == object:\n",
    "        print(col + \" is an object, converting to category...\")\n",
    "        data[col] = data[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34421e84-6b93-4694-8516-5e5fdd679923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject_id', 'label', 'charttime', 'glucose', 'next_glucose', 'mean_last3', 'std_last3', 'trend', 'BUN', 'Blood Pressure', 'Creatinine', 'Diastolic Blood Pressure', 'Heart Rate', 'Hemoglobin', 'O2 saturation pulseoxymetry', 'PTT', 'Respiratory Rate', 'Sodium (serum)', 'Systolic Blood Pressure', 'WBC']\n"
     ]
    }
   ],
   "source": [
    "print(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3992a761-94b1-46e7-b5cf-2b6108fda1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each feature column:\n",
      "glucose                            0\n",
      "mean_last3                         0\n",
      "std_last3                      41990\n",
      "trend                          41990\n",
      "BUN                              484\n",
      "Blood Pressure                   192\n",
      "Creatinine                       469\n",
      "Diastolic Blood Pressure         155\n",
      "Heart Rate                         0\n",
      "Hemoglobin                      1125\n",
      "O2 saturation pulseoxymetry       86\n",
      "PTT                            29312\n",
      "Respiratory Rate                 233\n",
      "Sodium (serum)                   465\n",
      "Systolic Blood Pressure          154\n",
      "WBC                             1167\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataY = data['label']\n",
    "dataX = data.drop(columns=['next_glucose', 'subject_id', 'label', 'charttime'])\n",
    "\n",
    "# Check for missing values in features\n",
    "print(\"Missing values in each feature column:\")\n",
    "print(dataX.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e073b9c-7b1c-4a65-aedb-74865aa448b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1514610 samples\n",
      "Validation set size: 210059 samples\n",
      "Test set size: 210059 samples\n"
     ]
    }
   ],
   "source": [
    "# impute missing values with the median as RandomForestClassifier cannot handle them\n",
    "dataX = dataX.fillna(dataX.median())\n",
    "\n",
    "# transform label column to numeric representation \n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(dataY)\n",
    "\n",
    "# store class labels\n",
    "class_labels = le.classes_\n",
    "dataY = le.transform(dataY)\n",
    "\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_data_X_, test_data_X, train_data_y_ , test_data_y = train_test_split(dataX, dataY, test_size=0.2, \n",
    "                                              shuffle=True,random_state=0)\n",
    "    \n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_data_X, val_data_X, train_data_y, val_data_y = train_test_split(train_data_X_, train_data_y_, test_size=0.25, \n",
    "                                            shuffle=True,random_state=0)\n",
    "\n",
    "# Apply SMOTE only to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "train_X_resampled, train_y_resampled = smote.fit_resample(train_data_X, train_data_y)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "train_X = train_X_resampled.to_numpy()\n",
    "train_y = train_y_resampled\n",
    "\n",
    "test_X = test_data_X.to_numpy()\n",
    "test_y = test_data_y\n",
    "\n",
    "val_X = val_data_X.to_numpy()\n",
    "val_y = val_data_y\n",
    "\n",
    "print(f\"Training set size: {train_X.shape[0]} samples\")\n",
    "print(f\"Validation set size: {val_X.shape[0]} samples\")\n",
    "print(f\"Test set size: {test_X.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04a665b-edea-4c0e-92db-a6c032a76584",
   "metadata": {},
   "source": [
    "### Initial model and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20d2a8-6ab6-4c2f-98e5-bb1737ddf59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# max_depth - max layers of a tree, reduces overfitting\n",
    "# n_estimators  - number of trees, balances model performance with computation time\n",
    "# balanced_subsample - gives more weight to minority classes\n",
    "# random_state - sets a consistent random seed\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=10, n_estimators=350, class_weight='balanced_subsample', random_state=0)\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a44a9a-8626-4a3f-aab3-f856e2f678de",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = clf.predict(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e02985-a758-4f76-9650-61d04882b459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(val_y, val_pred, target_names=class_labels))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(val_y, val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927026ba-db51-4898-b070-697faaf83a9c",
   "metadata": {},
   "source": [
    "### Grid Search \n",
    "Finds the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd584b66-8ed6-4b63-b4e5-6d8c2172d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [6, 8, 10, 12],\n",
    "    'n_estimators': [100, 250, 350, 500],\n",
    "    'class_weight': ['balanced_subsample', 'balanced']\n",
    "}\n",
    "\n",
    "# set up grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=0),\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# run grid search on the training data\n",
    "grid_search.fit(train_X, train_y)\n",
    "\n",
    "print(\"\\nBest parameters found:\", grid_search.best_params_)\n",
    "print(\"On validation F1 score is: {:.3f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e157e3-13cc-4493-b3ae-cdb8dffb3125",
   "metadata": {},
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ca3cc-54ff-4aa7-8568-f58fab8959b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and validation sets to retrain using the full training data\n",
    "full_train_X = np.concatenate((train_X, val_X), axis=0)\n",
    "full_train_y = np.concatenate((train_y, val_y), axis=0)\n",
    "\n",
    "# Extract best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train new model with best parameters\n",
    "final_clf = RandomForestClassifier(\n",
    "    max_depth=best_params['max_depth'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    class_weight=best_params['class_weight'],\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "final_clf.fit(full_train_X, full_train_y)\n",
    "\n",
    "# evaluate with test set\n",
    "test_f1 = get_f1_score(final_clf, test_X, test_y)\n",
    "\n",
    "print(\"Test F1 score: {:.3f}\".format(test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ab314-5e32-4740-b28c-63bab2af57e6",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "Finds the features most indicative of a sample belonging to one of the three classes. Helps with model interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04152614-c0c1-4201-b74f-c1afba341bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use the best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# get feature importances\n",
    "importances = best_model.feature_importances_\n",
    "feature_names = train_X.columns if hasattr(train_X, 'columns') else [f'Feature {i}' for i in range(train_X.shape[1])]\n",
    "\n",
    "# dataframe for easy sorting and plotting\n",
    "feat_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feat_df = feat_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# plot top 10 features\n",
    "feat_df.head(10).plot(kind='barh', x='Feature', y='Importance', legend=False, figsize=(8, 5))\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2591e0f9-0d27-4558-9857-ac464158dcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e2b1a-ca10-4733-9592-07136f880551",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
